source('~/Documents/rdemoecon113RKL.R')
source('~/Documents/rdemoecon113RKL.R')
source('~/Documents/rdemoecon113RKL.R')
source('~/Documents/rdemoecon113RKL.R')
View(wage1)
#Now, let's try some basic commands to learn more about our dataset.
#summary gives us some basic information about each variable inside our dataframe. It outputs in the console window below your R script window
summary(wage1)
#If you want to dive deeper into one element of your data do so in this format: dataset$variable
summary(wage1$wage)
#Next, let's look at some basic data analysis functions, like correllation, scatter plots, that kind of thing
#To create a scatter plot, we'll use scatter.smooth(). We're going to try to scatter wage and education, and it will output a plot in the bottom right window of r-studio
scatter.smooth(wage1$wage,wage1$educ) #scatter.smooth(y-axis,x-axis) There's other options, which you can find by googling the package
#next, let's try a correlation. We'll use the cor() function.
cor(wage1,use="complete.obs") #the use option here deletes null values. Good to do this, as cor() assumes a complete dataset and will throw an error if there's missing sections
#the output in console generates a correlation of every variable in the dataset. It's Massive! Let's try picking out a specific one instead
cor(wage1$wage, wage1$educ, use="complete.obs")
#now we only get the correlation between wage and education. However, we're not sure of the significance, because this package doesn't handle that. To do basic significance testing, we'll use cor.test()
cor.test(wage1$wage, wage1$educ)
#Regression stuff
#The lm() command in R is pretty similar to the reg command in STATA. We're gonna futz around with it for a bit.
lm(wage ~ educ+exper, data=wage1,)
#The first part is basically the equation for the regression you wish to run, with the "=" replaced with a ~. The second specifies where you're getting those variables. it makes it quicker to write.
summary(lm(wage ~ educ+exper, data=wage1,))
#Data manipulation
#Unlike Stata, R is rarely unhappy with lots of datasets loaded into dynamic memory at once. It makes it easy to chop, slice, and compare datasets in a quick and relatively freeform fashion.
#My personal strategy for working with datasets is to avoid editing the source one, and instead initalize new dataframes with the components I want.
#let's try creating a new dataframe, based on wage1, but only including wage, education, and experience
wagepartial <- data.frame(wage1$wage, wage1$educ, wage1$exper)
#Let's roll back to our earlier regression to keep experimenting with data manipulation.
#if you define an object as that summary, you can then pull useful specific coefficients out of it for later use!
testsummary <- summary(lm(wage ~ educ+exper, data=wage1,))
#you now have another data table generated from the summary of that linear model over in your regression window. Try pulling particular cells of that array by using testsummary[], with the term you want in the brackets
testsummary[["adj.r.squared"]]
testsummary[["coefficients"]]
#as you can see, each of these outputs generates a small table, which you can then define as another variable as well!
testcoefficients <-testsummary[["coefficients"]]
#you can then write any of these into a new .csv file by using the command "write.csv"
write.csv(testcoefficients,'testsummary.csv')
#Visualization
#Tables and numbers and coefficients are cool and all, but executives and nonspecialists, similarly to small children, like simple shapes and bright colors.
#We're going to go over how to produce those in R, using our handy, very powerful friend, ggplot2. ggplot2 is pretty complex, and can definitely be intimidating at first.
library(ggplot2)
ggplot(data= wage1, mapping =aes(educ,wage))+ geom_point()
#the function, ggplot, lets us define a dataset, our axis using the aes option, and then how to graph. we're using the default geom_point, but what happens when we change style or something?
ggplot(data= wage1, mapping =aes(educ,wage))+ geom_point(size=5, shape=4)
#neat, right? but we've got more powerful options available. What if we wanted to change the size of the point based on years of experience?
ggplot(data= wage1, mapping =aes(educ,wage))+ geom_point(aes(size=exper), shape=6)
#Neat!
#How about adding a regression line?
ggplot(data= wage1, mapping =aes(educ,wage))+ geom_point(aes(size=exper), shape=6)+geom_smooth(method=lm)
#what if we wanted to make a pi or bar chart to represent something like job demographics of our data
#First, we'd need to get a count of how many people working in each industry. We can use the sum command for that, and initialize a new data frame specifically for this purpose
SkilledTrades <- sum(wage1$trade)
Construction <- sum(wage1$construc)
NondurableManufacturing <- sum(wage1$ndurman)
TransportAndMisc <- sum(wage1$trcommpu)
Services <- sum(wage1$services)
#next, we create a dataframe and just throw all these variables into it
jobdemographics <- data.frame(SkilledTrades,Construction,NondurableManufacturing,TransportAndMisc,Services)
#so, now we've got this cool data frame, but we can't use it for our next task. We need to refactor it. The key columns and rows aren't very usable to create a bar chart.
#we could do this by hand, but I'm lazy. We're going to use a library called "tidyr" handle it.
#install.packages("tidyverse") #installing the library and loading it into the R script
library(tidyr)
#We're going to take our current data, and use the gather tool to take our column titles and convert them into a key column
jobdemographics <- gather(jobdemographics,'SkilledTrades','Construction','NondurableManufacturing','TransportAndMisc','Services', key="jobsector",value="jobcount")
#take a look at the difference now. Our data is now in a usable format, with actual, named key columns and row titles. Huzzah!
#Now, we're actually gonna make that bar chart.
jobdemochart <- ggplot(data=jobdemographics, aes(x=jobsector,y=jobcount))+geom_bar(stat="identity")
jobdemochart
#take a look at the output in your plot window. Looks pretty boring, yeah?
#time to add colors.
jobdemochart <- ggplot(data=jobdemographics, aes(x=jobsector,y=jobcount, color=jobsector, fill=jobsector))+geom_bar(stat="identity")
jobdemochart
#Wow! Pretty!
#For those of you who did well in trig, what happens when you convert cartesian coordinates to polar?
jobdemochart <- ggplot(data=jobdemographics, aes(x=jobsector,y=jobcount, color=jobsector, fill=jobsector))+geom_bar(stat="identity")+coord_polar("y",start=0)
jobdemochart
#neat, but not a pie chart in the conventional sense. What if we made a single "bar" and used the polar conversion?
jobdemochart <- ggplot(data=jobdemographics, aes(x="",y=jobcount, color=jobsector, fill=jobsector))+geom_bar(stat="identity")
jobdemochart
#neat. One bar, but still seperated by color. What happens if we convert this new updated single bar into polar?
jobdemochart <-(jobdemochart+coord_polar("y",start=0))
jobdemochart
#neat! Shapes and colors are present. Still too many numbers for small children/executives though, and that grey background could go away.
jobdemochart <- jobdemochart+theme_void()
jobdemochart
#No scary numbers, no ugly background. Just soft shapes and pleasant colors!
#No scary numbers, no ugly background. Just soft shapes and pleasant colors!
#Wow! Pretty!
#For those of you who did well in trig, what happens when you convert cartesian coordinates to polar?
jobdemochart <- ggplot(data=jobdemographics, aes(x=jobsector,y=jobcount, color=jobsector, fill=jobsector))+geom_bar(stat="identity")+coord_polar("y",start=0)
jobdemochart
#let's open our newly created dataframe jobdemographics, and see what it looks like
view(jobdemographics)
#let's open our newly created dataframe jobdemographics, and see what it looks like
View(jobdemographics)
library(haven)
wage1 <- read_dta("Documents/wage1.DTA")
View(wage1)
source('~/Documents/rdemoecon113RKL.R')
install.packages("tidyverse")
library(tidyverse)
detach("package:tidyverse", unload = TRUE)
library(tidyverse)
install.packages("writexl")
#Now, generalized linear models. A lot more useful, I can't go into the full details of how glms work, because I don't have math markup here, but rest assured it's kind of a piecewise polynomial with some penalty functions to deal with overfitting/smoothing.
genmodel -> glm(wage ~ educ+exper+tenure, data=wage1,family=possion())
#Now, generalized linear models. A lot more useful, I can't go into the full details of how glms work, because I don't have math markup here, but rest assured it's kind of a piecewise polynomial with some penalty functions to deal with overfitting/smoothing.
genmodel <- glm(wage ~ educ+exper+tenure, data=wage1,family=possion())
#Now, generalized linear models. A lot more useful, I can't go into the full details of how glms work, because I don't have math markup here, but rest assured it's kind of a piecewise polynomial with some penalty functions to deal with overfitting/smoothing.
genmodel <- glm(wage ~ educ+exper+tenure, data=wage1,family=poisson())
summary(genmodel)
econ216p5data. <- read.csv("~/Desktop/econ216p5data .csv")
View(econ216p5data.)
lm (y ~ x)
lm(y ~ x, data=econ216p5data.)
lm(y ~ x, data=econ216p5data.csv)
lm(y ~ x, data=econ216p5data.)
summary(lm(y ~ x, data=econ216p5data.))
summary(lm(y ~ x, data=econ216p5data.))
owid.covid.data <- read.csv("~/Desktop/owid-covid-data.csv", header=FALSE)
View(owid.covid.data)
library(foreign, lib.loc = "/usr/lib/R/library")
iris <- read.csv("~/Documents/RSTUFF/iris.csv")
View(iris)
source('~/.active-rstudio-document')
install.packages("e1071")
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
#summary of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
dotplot(results)
i <- 1
x <- 0
y <- 1
while (i<10)
{
z = x + y
print(paste(z,","))
x = y
y = z
i = i+1
}
primehunter <- function(n){
i <- 2:n
j <- 1
while (i[j] <= sqrt(n)) {
i <- i[i %% i[j] != 0 | i == i[j]]
j <- j+1
}
i
}
primehunter(12)
primehunter <- function(n){
i <- 2:n
j <- 1
while (i[j] <= sqrt(n)) {
i <- i[i %% i[j] != 0 | i == i[j]]
j <- j+1
}
i
}
primehunter(128)
primehunter(1280)
print(avec[1:3])
avec = c(3,5,7,9)
print(avec[1:3])
prime.SofE <- function(n){
x <- 2:n
isPrime <- c()
for (z in x+1){
isPrime[z] <- TRUE
}
i <- 2
while ( i*i <= n){
if(isPrime[i]==TRUE){
k <- i*i
while(K<=n){
isPrime[k]=FALSE
k= k+i
}
}
i =i+1
}
#REMOVE non primes
for (m in x){
if(isPrime[m])
x = x[x!= m]
}
return(x)
}
prime.SofE(12)
if(bool(isPrime[i])==TRUE){
k <- i*i
while(K<=n){
isPrime[k]=FALSE
k= k+i
}
}
if((isPrime[i]) IS TRUE){
k <- i*i
while(K<=n){
isPrime[k]=FALSE
k= k+i
}
}
i =i+1
if((isPrime[i]) == TRUE){
k <- i*i
while(K<=n){
isPrime[k]=FALSE
k= k+i
}
}
prime.SofE <- function(n){
x <- 2:n
isPrime <- c()
for (z in x+1){
isPrime[z] <- TRUE
}
i <- 2
while ( i*i <= n){
if((isPrime[i]) == TRUE){
k <- i*i
while(K<=n){
isPrime[k]=FALSE
k= k+i
}
}
i =i+1
}
#REMOVE non primes
for (m in x){
if(isPrime[m])
x = x[x!= m]
}
return(x)
}
prime.SofE(12)
##A
mynum <- list(numeric(10),numeric(10),numeric(10), numeric(10), numeric(10))
mychar <- list(str(quick), str(slow))
mychar <- list(("quick"),("slow"))
both <-mynum+mychar
both <-list(mynum,mychar)
type(both)
class(both)
#it's a list
##B
library(haven)
org_example <- read_dta("Documents/RSTUFF/org_example.dta")
View(org_example)
mydata <- df(org_example)
mydata <- (org_example)
str(mydata)
sum(mydata)
summarize(mydata)
names(mydata)
dim(mydata)
structure(mydata)
class(mydata$lfstat)
View(mydata)
View(mydata)
#actual machine learning stuff. First part is just a cross-validated control
control <- trainControl(method="cv", number = 10)
metric <- "Accuracy"
#model shotgun
#linear
set.seed(7)
fit.lda <- train(Species~., data=dataset, method="lda", metric=metric, trControl=control) #LDA
#nonlinear
set.seed(7)
fit.cart <- train(Species~., data=dataset, method="rpart", metric=metric, trControl=control) #rpart
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control) #Nearest neighbor
#weird stuff
set.seed(7)
fit.svm <- train(Species~., data=dataset, method="svmRadial", metric=metric, trControl=control) #SVM
set.seed(7)
fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control) #random forest
#summary of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
library(caret) #you're gonna have to install this library and a couple deps first
data(iris) #use the iris dataset internal to R
dataset <- iris #rename the iris dataset to dataset
index <- createDataPartition(dataset$Species, p=.80, list=FALSE) #creating an index that selects only 80% of the data
validation <- dataset[-index,] #using the stuff not included in the index to create a validation dataset/benchmark
dataset <- dataset[index,] #using the rest of the dataset as a dataset
#actual machine learning stuff. First part is just a cross-validated control
control <- trainControl(method="cv", number = 10)
metric <- "Accuracy"
#model shotgun
#linear
set.seed(7)
fit.lda <- train(Species~., data=dataset, method="lda", metric=metric, trControl=control) #LDA
#nonlinear
set.seed(7)
fit.cart <- train(Species~., data=dataset, method="rpart", metric=metric, trControl=control) #rpart
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control) #Nearest neighbor
#weird stuff
set.seed(7)
fit.svm <- train(Species~., data=dataset, method="svmRadial", metric=metric, trControl=control) #SVM
set.seed(7)
fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control) #random forest
#summary of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
control <- trainControl(method="cv", number = 10)
metric <- "Accuracy"
#model shotgun
#linear
set.seed(7)
fit.lda <- train(Species~., data=dataset, method="lda", metric=metric, trControl=control) #LDA
#nonlinear
set.seed(7)
fit.cart <- train(Species~., data=dataset, method="rpart", metric=metric, trControl=control) #rpart
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control) #Nearest neighbor
#weird stuff
set.seed(7)
fit.svm <- train(Species~., data=dataset, method="svmRadial", metric=metric, trControl=control) #SVM
set.seed(7)
fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control) #r
library(caret) #you're gonna have to install this library and a couple deps first
data(iris) #use the iris dataset internal to R
install.packages("carat")
install.packages(c("backports", "BH", "broom", "callr", "cli", "clipr", "colorspace", "cpp11", "crayon", "data.table", "DBI", "dbplyr", "digest", "dplyr", "fansi", "forcats", "ggplot2", "hms", "htmltools", "isoband", "jsonlite", "knitr", "labeling", "lubridate", "magrittr", "openssl", "pillar", "pkgbuild", "pROC", "processx", "ps", "R6", "Rcpp", "readr", "reprex", "rlang", "rmarkdown", "rprojroot", "rstudioapi", "SQUAREM", "testthat", "tibble", "tinytex", "vctrs", "withr", "xfun"))
install.packages("carat", dependencies=TRUE)
install.packages("knitr")
install.packages("carat", dependencies="TRUE")
install.packages("carat")
install.packages("carat")
sudo
install.packages("carat")
install.packages("caret")
library(ISLR)
library(e1071)
set.seed(1)
data(Caravan)
y<-Caravan$Purchase # outcome variable
X<-Caravan[,-86] # covariates
# Setting train and test sets
test<-1:500
train.y<-y[-test]
train.X<-X[-test,]
test.y<-y[test]
test.X<-X[test,]
train.data<-data.frame(x=train.X, y=train.y)
test.data<-data.frame(x=test.X, y=test.y)
# Train SVM
costs<-seq(0.01, 2, length.out = 200)
test.accuracy<-costs
for(i in 1:length(costs)){
print(i)
# Fit SVM with polynomial kernel.
svmfit<-svm(y~., data=train.data, kernel="radial",gamma=2, cost = costs[i])
# Predict labels for test data
test.ypred<-predict(svmfit,test.data)
# Test sample accuracy
test.accuracy[i]<-mean(test.ypred==test.data$y)
}
test.bestcost<-costs[which.max(test.accuracy)]
print(test.bestcost)
View(svmfit)
View(Caravan)
#5
#TBD
library(ISLR)
data("OJ")
force(OJ)
View(OJ)
y<-OJ$Purchase # outcome variable
X<-OJ[,-86] # covariates
View(X)
X<-OJ[2:18,] # covariates
X<-OJ[,2:18] # covariates
library(e1071)
data("OJ")
y<-OJ$Purchase # outcome variable
X<-OJ[,2:18] # covariates
# Setting train and test sets
test<-1:500
train.y<-y[-test]
train.X<-X[-test,]
test.y<-y[test]
test.X<-X[test,]
train.data<-data.frame(x=train.X, y=train.y)
test.data<-data.frame(x=test.X, y=test.y)
# Train SVM
costs<-seq(0.01, 2, length.out = 200)
test.accuracy<-costs
for(i in 1:length(costs)){
print(i)
# Fit SVM with polynomial kernel.
svmfit<-svm(y~., data=train.data, kernel="radial",gamma=2, cost = costs[i])
# Predict labels for test data
test.ypred<-predict(svmfit,test.data)
# Test sample accuracy
test.accuracy[i]<-mean(test.ypred==test.data$y)
}
test.bestcost<-costs[which.max(test.accuracy)]
print(test.bestcost)
for(i in costs){
svmfit<-svm(y~.,
data=train.data,
kernel="radial",gamma=2,
cost = costs[i])
# Predict labels for test data
test.ypred<-predict(svmfit,test.data)
# Test sample accuracy
test.accuracy[i]<-mean(test.ypred==test.data$y)
}
test.bestcost<-costs[which.max(test.accuracy)]
print(test.bestcost)
gc()
source('~/Documents/Thesis/econ217hw4.R')
library(keras)
set.seed(1)
mnist <- dataset_mnist()
# training set
x_train <- mnist$train$x
y_train <- mnist$train$y
# test set
x_test <- mnist$test$x
y_test <- mnist$test$y
# Reshape covariates so that we have 784 = 28*28 columns per image.
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# Rescale covariates, dividing by maximum value of x_train.
x_train <- x_train / 255
x_test <- x_test / 255
# Make outcomes categorical.
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
testl1s<-c(0.00001,0.0001,.001,.005,.01,.05)
scorelist<- c() #should probably init this at the length I need it, but w/e
for(i in (testl1s)){
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784), kernel_regularizer = regularizer_l1(l = i)) %>%
layer_dense(units = 128, activation = 'relu', kernel_regularizer = regularizer_l1(l = i)) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'accuracy'
)
history <- model %>% fit(x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2)
score <- model %>% evaluate(x_test, y_test)
print(i)
print(score)
scorelist<-append(scorelist, score[2])
}
testl1s<-c(0.00001,0.0001,.001,.005,.01,.05)
scorelist<- c() #should probably init this at the length I need it, but w/e
for(i in (testl1s)){
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784), kernel_regularizer = regularizer_l1(l = i)) %>%
layer_dense(units = 128, activation = 'relu', kernel_regularizer = regularizer_l1(l = i)) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'accuracy'
)
history <- model %>% fit(x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2)
score <- model %>% evaluate(x_test, y_test)
print(i)
print(score)
scorelist<-append(scorelist, score[2])
}
gc()
source('~/Documents/Thesis/bycounty.R')
